{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27ecrRm8Giod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "3efd3f5b-7a76-4180-c5db-aefed23ba38f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-219462ea93c8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    4from google.colab import drive\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
          ]
        }
      ],
      "source": [
        "4from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xgh12HXJUbwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74541197-1d79-40ff-bf13-57b5f804f353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxmY4izoUzDs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3b373a-2fd9-4dc1-af4f-81ebe9c5d124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyiuDzOeVMU2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pegbKRPjUgmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f1bb49-ffc8-4e47-c02a-fb516f16cbaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShyVkQ7oV7Ll"
      },
      "outputs": [],
      "source": [
        "# model_path = '/content/drive/MyDrive/masters/araber_putourch/bert-base-arabertv01'\n",
        "# model_path = '/content/drive/MyDrive/masters/araber_putourch/bert-base-arabert'\n",
        "# model_path = '/content/drive/MyDrive/bert-base-arabert'\n",
        "#model_path = 'UBC-NLP/ARBERT'\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "# # no seg\n",
        "# model_path = 'aubmindlab/bert-large-arabertv02'\n",
        "# # seg\n",
        "# model_path ='aubmindlab/bert-base-arabertv2'\n",
        "# # seg\n",
        "# model_path = 'aubmindlab/bert-large-arabertv2'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lE4RVzLUxT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2173d7-6944-4d73-87b3-27f415a3461e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN-o_A8HTPvg"
      },
      "outputs": [],
      "source": [
        "benchmark_path = \"/content/drive/MyDrive/Colab Notebooks/Arabic-word-sense-disambiguation-bench-mark-main/new_df_pairs.parquet\"\n",
        "df = pd.read_parquet(benchmark_path)\n",
        "# df = pd.read_parquet(\"/content/drive/MyDrive/masters/new_df_pairs_with_all_postive_ex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MUeYwCecUC5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "c6d4a4e0-f486-4245-91f4-03b838fa4efc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID   word                                                def  \\\n",
              "0       9761    آثر                   : آثر الشيء فضله واختاره   { } .   \n",
              "1       9761    آثر       : آثره على نفسه: قدمه واختصه بالخير :- { } .   \n",
              "2       9762    آثر                   : آثر الشيء فضله واختاره   { } .   \n",
              "3       9762    آثر       : آثره على نفسه: قدمه واختصه بالخير :- { } .   \n",
              "4       8364    آخذ                : آخذ الرجل عاتبه، لامه وعابه   { }   \n",
              "...      ...    ...                                                ...   \n",
              "31093  54579  يومية             : مصدر صناعي من يوم: أجر العامل اليومي   \n",
              "31094  54580  يومية  : اسم مؤنث منسوب إلى يوم:   جريدة يومية: تصدر ...   \n",
              "31095  54580  يومية             : مصدر صناعي من يوم: أجر العامل اليومي   \n",
              "31096  54581  يومية  : اسم مؤنث منسوب إلى يوم:   جريدة يومية: تصدر ...   \n",
              "31097  54581  يومية  : السجل الذي يدون فيه الشخص انطباعاته يوميا، ا...   \n",
              "\n",
              "                                                    ex  label  \n",
              "0                              بل تؤثرون الحياة الدنيا      1  \n",
              "1                              بل تؤثرون الحياة الدنيا      0  \n",
              "2                 ويؤثرون على أنفسهم ولو كان بهم خصاصة      0  \n",
              "3                 ويؤثرون على أنفسهم ولو كان بهم خصاصة      1  \n",
              "4      قال لا تؤاخذني بما نسيت ولا ترهقني من أمري عسرا      1  \n",
              "...                                                ...    ...  \n",
              "31093                                 :-عاملة يومية، -      0  \n",
              "31094                                 :-يعمل باليومية.      0  \n",
              "31095                                 :-يعمل باليومية.      1  \n",
              "31096                                 :-دون يومياته، -      0  \n",
              "31097                                 :-دون يومياته، -      1  \n",
              "\n",
              "[31098 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-0978abf0-7c60-4b1c-9254-90e2b1f7dc50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>word</th>\n",
              "      <th>def</th>\n",
              "      <th>ex</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9761</td>\n",
              "      <td>آثر</td>\n",
              "      <td>: آثر الشيء فضله واختاره   { } .</td>\n",
              "      <td>بل تؤثرون الحياة الدنيا</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9761</td>\n",
              "      <td>آثر</td>\n",
              "      <td>: آثره على نفسه: قدمه واختصه بالخير :- { } .</td>\n",
              "      <td>بل تؤثرون الحياة الدنيا</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9762</td>\n",
              "      <td>آثر</td>\n",
              "      <td>: آثر الشيء فضله واختاره   { } .</td>\n",
              "      <td>ويؤثرون على أنفسهم ولو كان بهم خصاصة</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9762</td>\n",
              "      <td>آثر</td>\n",
              "      <td>: آثره على نفسه: قدمه واختصه بالخير :- { } .</td>\n",
              "      <td>ويؤثرون على أنفسهم ولو كان بهم خصاصة</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8364</td>\n",
              "      <td>آخذ</td>\n",
              "      <td>: آخذ الرجل عاتبه، لامه وعابه   { }</td>\n",
              "      <td>قال لا تؤاخذني بما نسيت ولا ترهقني من أمري عسرا</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31093</th>\n",
              "      <td>54579</td>\n",
              "      <td>يومية</td>\n",
              "      <td>: مصدر صناعي من يوم: أجر العامل اليومي</td>\n",
              "      <td>:-عاملة يومية، -</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31094</th>\n",
              "      <td>54580</td>\n",
              "      <td>يومية</td>\n",
              "      <td>: اسم مؤنث منسوب إلى يوم:   جريدة يومية: تصدر ...</td>\n",
              "      <td>:-يعمل باليومية.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31095</th>\n",
              "      <td>54580</td>\n",
              "      <td>يومية</td>\n",
              "      <td>: مصدر صناعي من يوم: أجر العامل اليومي</td>\n",
              "      <td>:-يعمل باليومية.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31096</th>\n",
              "      <td>54581</td>\n",
              "      <td>يومية</td>\n",
              "      <td>: اسم مؤنث منسوب إلى يوم:   جريدة يومية: تصدر ...</td>\n",
              "      <td>:-دون يومياته، -</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31097</th>\n",
              "      <td>54581</td>\n",
              "      <td>يومية</td>\n",
              "      <td>: السجل الذي يدون فيه الشخص انطباعاته يوميا، ا...</td>\n",
              "      <td>:-دون يومياته، -</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31098 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0978abf0-7c60-4b1c-9254-90e2b1f7dc50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d2686b86-2fd9-4a2b-a610-fc9fed63b35f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2686b86-2fd9-4a2b-a610-fc9fed63b35f')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d2686b86-2fd9-4a2b-a610-fc9fed63b35f button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0978abf0-7c60-4b1c-9254-90e2b1f7dc50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0978abf0-7c60-4b1c-9254-90e2b1f7dc50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdSsPPJuWfSE"
      },
      "outputs": [],
      "source": [
        "new_df_pairs = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsqaoKPYWBqF"
      },
      "outputs": [],
      "source": [
        "new_df_pairs['input_sentence'] =new_df_pairs.apply(lambda row : '[CLS] '+row['ex']+' [SEP] '+row['def'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRmrzSMbTZ4N"
      },
      "outputs": [],
      "source": [
        "new_df_pairs.sample(frac=1)\n",
        "msk = np.random.rand(len(new_df_pairs)) < 0.8\n",
        "new_train = new_df_pairs[msk]\n",
        "test = new_df_pairs[~msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wze6reFVH2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c74e3e1-e812-418e-be62-b85a547c8722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  [CLS] بل تؤثرون الحياة الدنيا [SEP] : آثر الشيء فضله واختاره   { } .\n",
            "Token IDs: [2, 549, 1275, 527, 319, 1725, 6449, 3, 31, 4164, 8110, 6819, 195, 29413, 195, 96, 98, 20]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "# For every sentence...\n",
        "for sent in new_train['input_sentence']:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', new_train['input_sentence'].iloc[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vr02ZSP922lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e051c2f3-7987-40b4-fe68-54a16d192e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31098\n",
            "24956\n",
            "6142\n",
            "           ID   word                                                def  \\\n",
            "5        8364    آخذ  : آخذه بذنبه/ آخذه على ذنبه: عاقبه عليه وجازاه...   \n",
            "16      37567    آخر  : آخر :-  جمع آخرون وأواخر: عكس أول في الرتبة ...   \n",
            "17      37567    آخر              : مختلف، مغاير أو بمعنى غيره :- { } .   \n",
            "18      37571   آخرة            : آخرة :-  جمع أواخر: مؤنث آخر:   { } .   \n",
            "22      37573   آخرة            : آخرة :-  جمع أواخر: مؤنث آخر:   { } .   \n",
            "...       ...    ...                                                ...   \n",
            "31064    4942  يميني                  : اسم منسوب إلى يمين1: عكسه يساري   \n",
            "31070  264414    ينع            : ينع الثمر نضج، طاب وحان قطافه   { } .   \n",
            "31077   93550  يهودي                : يهودي :-  جمع يهود: (انظر: هـ و د   \n",
            "31095   54580  يومية             : مصدر صناعي من يوم: أجر العامل اليومي   \n",
            "31096   54581  يومية  : اسم مؤنث منسوب إلى يوم:   جريدة يومية: تصدر ...   \n",
            "\n",
            "                                                      ex  label  \\\n",
            "5        قال لا تؤاخذني بما نسيت ولا ترهقني من أمري عسرا      0   \n",
            "16                                  ثم أنشأناه خلقا ءاخر      0   \n",
            "17                                  ثم أنشأناه خلقا ءاخر      1   \n",
            "18                            له الحمد في الأولى والآخرة      1   \n",
            "22                           :-أرى احمرارا في آخرة عينك.      0   \n",
            "...                                                  ...    ...   \n",
            "31064  :-تفوق أعداد اليمينيين في البرلمان أعداد اليسا...      0   \n",
            "31070                     انظروا إلى ثمره إذا أثمر وينعه      1   \n",
            "31077                                          - يهودي).      1   \n",
            "31095                                   :-يعمل باليومية.      1   \n",
            "31096                                   :-دون يومياته، -      0   \n",
            "\n",
            "                                          input_sentence  \n",
            "5      [CLS] قال لا تؤاخذني بما نسيت ولا ترهقني من أم...  \n",
            "16     [CLS] ثم أنشأناه خلقا ءاخر [SEP] : آخر :-  جمع...  \n",
            "17     [CLS] ثم أنشأناه خلقا ءاخر [SEP] : مختلف، مغاي...  \n",
            "18     [CLS] له الحمد في الأولى والآخرة [SEP] : آخرة ...  \n",
            "22     [CLS] :-أرى احمرارا في آخرة عينك. [SEP] : آخرة...  \n",
            "...                                                  ...  \n",
            "31064  [CLS] :-تفوق أعداد اليمينيين في البرلمان أعداد...  \n",
            "31070  [CLS] انظروا إلى ثمره إذا أثمر وينعه [SEP] : ي...  \n",
            "31077  [CLS] - يهودي). [SEP] : يهودي :-  جمع يهود: (ا...  \n",
            "31095  [CLS] :-يعمل باليومية. [SEP] : مصدر صناعي من ي...  \n",
            "31096  [CLS] :-دون يومياته، - [SEP] : اسم مؤنث منسوب ...  \n",
            "\n",
            "[6142 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "print(len(new_df_pairs))\n",
        "print(len(new_train))\n",
        "print(len(test))\n",
        "print(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhk2en_v3gl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef55d1ff-2660-4c09-f0f8-3e5eef8bfbc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  [CLS] قال لا تؤاخذني بما نسيت ولا ترهقني من أمري عسرا [SEP] : آخذه بذنبه/ آخذه على ذنبه: عاقبه عليه وجازاه :- { } .\n",
            "Token IDs: [2, 549, 1275, 527, 319, 1725, 6449, 3, 31, 4164, 8110, 6819, 195, 29413, 195, 96, 98, 20]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids_test = []\n",
        "# For every sentence...\n",
        "for sent in test['input_sentence']:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = False, # Add '[CLS]' and '[SEP]'\n",
        "                        # This function also supports truncation and conversion\n",
        "                        # to pytorch tensors, but we need to do padding, so we\n",
        "                        # can't use these features :( .\n",
        "                        #max_length = 128,          # Truncate all sentences.\n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids_test.append(encoded_sent)\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', test['input_sentence'].iloc[0])\n",
        "print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzBpdzEqXK9y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da36f9a0-a3df-4d52-9f5f-23c2b689fe9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max sentence length:  333\n"
          ]
        }
      ],
      "source": [
        "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a9C1H6HXd6G"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 492"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq1PzXFCPUDQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99805c7c-8812-4423-eedc-f4eb2e9101be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras_preprocessing in /usr/local/lib/python3.10/dist-packages (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install keras_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jw7xb--XaGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdfb9018-b171-4779-bbea-9f21ee7e6739"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 400 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\\Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 400\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol7oZ2iXxMwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bcc0d6-a65a-437d-ef15-4ed425fbf477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 400 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\\Done.\n"
          ]
        }
      ],
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "# from keras.preprocessing.sequence import pad_sequences\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "# MAX_LEN = 400\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids_test = pad_sequences(input_ids_test, maxlen=MAX_LEN, dtype=\"long\",\n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "print('\\Done.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrTrM8rJWh5A"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ-SSJmRxh-x"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks_test = []\n",
        "# For each sentence...\n",
        "for sent in input_ids_test:\n",
        "\n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "\n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks_test.append(att_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqCK5tdJXx9I"
      },
      "outputs": [],
      "source": [
        "train_labels = new_train['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBL94rU_x4qT"
      },
      "outputs": [],
      "source": [
        "test_labels = test['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88C4MuLfZlip"
      },
      "outputs": [],
      "source": [
        "# # Use train_test_split to split our data into train and validation sets for\n",
        "# # training\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# # Use 90% for training and 10% for validation.\n",
        "# train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, df_labels,\n",
        "#                                                             random_state=2018, test_size=0.2)\n",
        "# # Do the same for the masks.\n",
        "# train_masks, validation_masks, _, _ = train_test_split(attention_masks, df_labels,\n",
        "#                                              random_state=2018, test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVFoWhuLXmEI"
      },
      "outputs": [],
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype\n",
        "# for our model.\n",
        "train_inputs = torch.tensor(input_ids)\n",
        "validation_inputs = torch.tensor(input_ids_test)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(test_labels)\n",
        "train_masks = torch.tensor(attention_masks)\n",
        "validation_masks = torch.tensor(attention_masks_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IM1asnmYLMZ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "# The DataLoader needs to know our batch size for training, so we specify it\n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "batch_size = 8\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NediaOn4Yf5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f8a425b-833b-425d-8038-37ddca32427f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iQU62eNYTWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e6d454-74bb-4cb9-aceb-625b2c0d824c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "# linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "  model_path, # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "# model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hBIy8NRqwB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e882b1b5-e48a-4d32-cdbb-d98532d5a676"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(64000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-nuKWjwdRTI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac877174-909f-47b8-eff8-83a4533c9ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (64000, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ],
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nAAdJF8dgjC"
      },
      "outputs": [],
      "source": [
        "params = list(model.named_parameters())\n",
        "for name, param in params[-5:]:\n",
        "    # print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    param.requires_grad = True\n",
        "for name, param in params[:-6]:\n",
        "    # print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhB44_TuYjWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49f4d7f9-16f7-4e1e-be20-d0c1aa2ec7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 1\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKJ2hy06Ys6j"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoF_VONuY2zM"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import datetime\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mK9ZqdRY6Jr",
        "outputId": "31d5b8c9-4ffc-47fa-970e-ee84c2bda6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n",
            "  Batch    40  of  3,120.    Elapsed: 0:00:21.\n",
            "  Batch    80  of  3,120.    Elapsed: 0:00:43.\n",
            "  Batch   120  of  3,120.    Elapsed: 0:01:05.\n",
            "  Batch   160  of  3,120.    Elapsed: 0:01:27.\n",
            "  Batch   200  of  3,120.    Elapsed: 0:01:49.\n",
            "  Batch   240  of  3,120.    Elapsed: 0:02:11.\n",
            "  Batch   280  of  3,120.    Elapsed: 0:02:33.\n",
            "  Batch   320  of  3,120.    Elapsed: 0:02:55.\n",
            "  Batch   360  of  3,120.    Elapsed: 0:03:17.\n",
            "  Batch   400  of  3,120.    Elapsed: 0:03:39.\n",
            "  Batch   440  of  3,120.    Elapsed: 0:04:01.\n",
            "  Batch   480  of  3,120.    Elapsed: 0:04:23.\n",
            "  Batch   520  of  3,120.    Elapsed: 0:04:46.\n",
            "  Batch   560  of  3,120.    Elapsed: 0:05:08.\n",
            "  Batch   600  of  3,120.    Elapsed: 0:05:30.\n",
            "  Batch   640  of  3,120.    Elapsed: 0:05:52.\n",
            "  Batch   680  of  3,120.    Elapsed: 0:06:14.\n",
            "  Batch   720  of  3,120.    Elapsed: 0:06:36.\n",
            "  Batch   760  of  3,120.    Elapsed: 0:06:58.\n",
            "  Batch   800  of  3,120.    Elapsed: 0:07:20.\n",
            "  Batch   840  of  3,120.    Elapsed: 0:07:42.\n",
            "  Batch   880  of  3,120.    Elapsed: 0:08:04.\n",
            "  Batch   920  of  3,120.    Elapsed: 0:08:26.\n",
            "  Batch   960  of  3,120.    Elapsed: 0:08:48.\n",
            "  Batch 1,000  of  3,120.    Elapsed: 0:09:10.\n",
            "  Batch 1,040  of  3,120.    Elapsed: 0:09:32.\n",
            "  Batch 1,080  of  3,120.    Elapsed: 0:09:54.\n",
            "  Batch 1,120  of  3,120.    Elapsed: 0:10:16.\n",
            "  Batch 1,160  of  3,120.    Elapsed: 0:10:38.\n",
            "  Batch 1,200  of  3,120.    Elapsed: 0:11:01.\n",
            "  Batch 1,240  of  3,120.    Elapsed: 0:11:23.\n",
            "  Batch 1,280  of  3,120.    Elapsed: 0:11:45.\n",
            "  Batch 1,320  of  3,120.    Elapsed: 0:12:07.\n",
            "  Batch 1,360  of  3,120.    Elapsed: 0:12:29.\n",
            "  Batch 1,400  of  3,120.    Elapsed: 0:12:51.\n",
            "  Batch 1,440  of  3,120.    Elapsed: 0:13:13.\n",
            "  Batch 1,480  of  3,120.    Elapsed: 0:13:35.\n",
            "  Batch 1,520  of  3,120.    Elapsed: 0:13:57.\n",
            "  Batch 1,560  of  3,120.    Elapsed: 0:14:19.\n",
            "  Batch 1,600  of  3,120.    Elapsed: 0:14:41.\n",
            "  Batch 1,640  of  3,120.    Elapsed: 0:15:03.\n",
            "  Batch 1,680  of  3,120.    Elapsed: 0:15:25.\n",
            "  Batch 1,720  of  3,120.    Elapsed: 0:15:47.\n",
            "  Batch 1,760  of  3,120.    Elapsed: 0:16:10.\n",
            "  Batch 1,800  of  3,120.    Elapsed: 0:16:32.\n",
            "  Batch 1,840  of  3,120.    Elapsed: 0:16:54.\n",
            "  Batch 1,880  of  3,120.    Elapsed: 0:17:16.\n",
            "  Batch 1,920  of  3,120.    Elapsed: 0:17:38.\n",
            "  Batch 1,960  of  3,120.    Elapsed: 0:18:00.\n",
            "  Batch 2,000  of  3,120.    Elapsed: 0:18:22.\n",
            "  Batch 2,040  of  3,120.    Elapsed: 0:18:44.\n",
            "  Batch 2,080  of  3,120.    Elapsed: 0:19:06.\n",
            "  Batch 2,120  of  3,120.    Elapsed: 0:19:28.\n",
            "  Batch 2,160  of  3,120.    Elapsed: 0:19:50.\n",
            "  Batch 2,200  of  3,120.    Elapsed: 0:20:12.\n",
            "  Batch 2,240  of  3,120.    Elapsed: 0:20:34.\n",
            "  Batch 2,280  of  3,120.    Elapsed: 0:20:56.\n",
            "  Batch 2,320  of  3,120.    Elapsed: 0:21:18.\n",
            "  Batch 2,360  of  3,120.    Elapsed: 0:21:40.\n",
            "  Batch 2,400  of  3,120.    Elapsed: 0:22:02.\n",
            "  Batch 2,440  of  3,120.    Elapsed: 0:22:24.\n",
            "  Batch 2,480  of  3,120.    Elapsed: 0:22:47.\n",
            "  Batch 2,520  of  3,120.    Elapsed: 0:23:09.\n",
            "  Batch 2,560  of  3,120.    Elapsed: 0:23:31.\n",
            "  Batch 2,600  of  3,120.    Elapsed: 0:23:53.\n",
            "  Batch 2,640  of  3,120.    Elapsed: 0:24:15.\n",
            "  Batch 2,680  of  3,120.    Elapsed: 0:24:37.\n",
            "  Batch 2,720  of  3,120.    Elapsed: 0:24:59.\n",
            "  Batch 2,760  of  3,120.    Elapsed: 0:25:21.\n",
            "  Batch 2,800  of  3,120.    Elapsed: 0:25:43.\n",
            "  Batch 2,840  of  3,120.    Elapsed: 0:26:05.\n",
            "  Batch 2,880  of  3,120.    Elapsed: 0:26:27.\n",
            "  Batch 2,920  of  3,120.    Elapsed: 0:26:49.\n",
            "  Batch 2,960  of  3,120.    Elapsed: 0:27:11.\n",
            "  Batch 3,000  of  3,120.    Elapsed: 0:27:33.\n",
            "  Batch 3,040  of  3,120.    Elapsed: 0:27:55.\n",
            "  Batch 3,080  of  3,120.    Elapsed: 0:28:17.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:28:39\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation took: 0:02:24\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    # Put the model into training mode. Don't be mislead--the call to\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because\n",
        "        # accumulating the gradients is \"convenient while training RNNs\".\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here:\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    labels=b_labels)\n",
        "\n",
        "        # The call to `model` always returns a tuple, so we need to pull the\n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value\n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here:\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids,\n",
        "                            token_type_ids=None,\n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlRUADdkjnb0"
      },
      "outputs": [],
      "source": [
        "# Set the batch size.\n",
        "batch_size = 32\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhLUqIaojx0o",
        "outputId": "f14e8742-57bd-4c41-d1a7-4e0f96afad1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 6,142 test sentences...\n",
            "DONE.\n"
          ]
        }
      ],
      "source": [
        "prediction_inputs = validation_inputs\n",
        "# =validation_masks\n",
        "# =validation_labels\n",
        "# Prediction on test set\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "# Predict\n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=None,\n",
        "                      attention_mask=b_input_mask)\n",
        "  logits = outputs[0]\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "print('DONE.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDzwlP43kTlh",
        "outputId": "f2f2f92f-44cf-4590-814a-af295d18da07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive samples: 15549 of 31098 (50.00%)\n"
          ]
        }
      ],
      "source": [
        "print('Positive samples: %d of %d (%.2f%%)' % (df.label.sum(), len(df.label), (df.label.sum() / len(df.label) * 100.0)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kURQFsdqk92f",
        "outputId": "f6da94c3-d05d-4dda-ccab-179dd8a86d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "matthews_set = []\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "\n",
        "  # Calculate and store the coef for this batch.\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uaTe1e1lijw",
        "outputId": "57609a1b-d804-47d9-9ea5-ff9fb9e0f8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 0.614\n"
          ]
        }
      ],
      "source": [
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "print('MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY1stbsnJASz"
      },
      "outputs": [],
      "source": [
        "# y_true.sum()/len(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKLRVy8Xl47S",
        "outputId": "8bacbb5f-a10d-4f20-e8d0-ea564774f79d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.798599804623901, 0.798599804623901, 0.798599804623901, None)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "y_true = flat_predictions\n",
        "y_pred = flat_true_labels\n",
        "# precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
        "# precision_recall_fscore_support(y_true, y_pred, average='weighted')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-JKgtWEmSUj",
        "outputId": "1bb6cf3d-5024-4c4e-d64f-5b7c4f86ca54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.67941176, 0.91693705]),\n",
              " array([0.89036403, 0.74231678]),\n",
              " array([0.77071362, 0.82043838]),\n",
              " array([2335, 3807]))"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ],
      "source": [
        "precision_recall_fscore_support(y_true, y_pred, average=None,\n",
        "labels=[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2l5__5oWeIN"
      },
      "source": [
        "# END of pyorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqmbnGFIhfyw"
      },
      "outputs": [],
      "source": [
        "  # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "with torch.no_grad():\n",
        "    # Forward pass, calculate logit predictions.\n",
        "    # This will return the logits rather than the loss because we have\n",
        "    # not provided labels.\n",
        "    # token_type_ids is the same as the \"segment ids\", which\n",
        "    # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "    # The documentation for this `model` function is here:\n",
        "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "    outputs = model(b_input_ids,\n",
        "                    token_type_ids=None,\n",
        "                    attention_mask=b_input_mask)\n",
        "\n",
        "# Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "# values prior to applying an activation function like the softmax.\n",
        "logits = outputs[0]\n",
        "# Move logits and labels to CPU\n",
        "logits = logits.detach().cpu().numpy()\n",
        "label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "# Calculate the accuracy for this batch of test sentences.\n",
        "tmp_eval_accuracy = flat_accuracy(logits, label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYEFqbiuZeIM"
      },
      "outputs": [],
      "source": [
        "new_df_pairs.sample(frac=1)\n",
        "msk = np.random.rand(len(new_df_pairs)) < 0.8\n",
        "train = new_df_pairs[msk]\n",
        "test = new_df_pairs[~msk]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SYAO6NSZw02"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras import regularizers\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ2S436OVtra"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer):\n",
        "  num_examples = len(glue_dict[\"sentence1\"])\n",
        "\n",
        "  sentence1 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "      for s in glue_dict[\"sentence1\"]])\n",
        "  sentence2 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "       for s in glue_dict[\"sentence2\"]])\n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s1 = tf.zeros_like(sentence1)\n",
        "  type_s2 = tf.ones_like(sentence2)\n",
        "  input_type_ids = tf.concat(\n",
        "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs\n",
        "dict_train = {'sentence1':train[\"def\"],\"sentence2\":train[\"ex\"]}\n",
        "train_data_set = bert_encode(dict_train, tokenizer)\n",
        "train_labels = train['label']\n",
        "train_labels = tf.convert_to_tensor(train_labels.tolist())\n",
        "\n",
        "# glue_test = bert_encode(glue['test'], tokenizer)\n",
        "# glue_test_labels  = glue['test']['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZuW6Ik2b4yQ"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'model.bin')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ05p2K_faDv",
        "outputId": "02bf8c7a-d40c-4d72-8426-e53941b8905e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.to_json_file(\"config.json\")\n",
        "model.config.to_json_file(\"args.json\")"
      ],
      "metadata": {
        "id": "3pR4--A-Fqum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiSScxxnmBuX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "030597c2-7194-405c-d442-78990d783b1c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-112-34ff69939704>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'config.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'args.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_with_config_and_args.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-34ff69939704>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, config, args, filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'stat_dict'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "def save_model(model, config, args, filename):\n",
        "  with open(filename, 'wb') as f:\n",
        "    torch.save(model.stat_dict(), f)\n",
        "    json.dump(config, f)\n",
        "    json.dump(args, f)\n",
        "model = torch.load('model.bin')\n",
        "config = json.load(open('config.json'))\n",
        "args = json.load(open('args.json'))\n",
        "save_model(model, config, args, 'model_with_config_and_args.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBjefNHq8N2c",
        "outputId": "3a81e460-24d9-439c-8138-71c663ee420f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)  # assuming 2 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GopNEfAMzjbv",
        "outputId": "5a5147d2-4586-41ab-8f80-7c6c1e44151d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1])\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)  # assuming 2 classes\n",
        "#true example output\n",
        "sentence1 = \"بل تؤثرون الحياة الدنيا\"\n",
        "sentence2 = \"آثره على نفسه: قدمه واختصه بالخير\"\n",
        "\n",
        "inputs = tokenizer(sentence1, sentence2, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWzbB1aLGGdZ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "import torch\n",
        "\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)  # assuming 2 classes\n",
        "\n",
        "# Load your dataset with input sentences and true labels\n",
        "dataset_sentences = [\"بل تؤثرون الحياة الدنيا\", \"آثره على نفسه: قدمه واختصه بالخير\"]\n",
        "dataset_labels = [0, 1]  # Example true labels, modify based on your dataset\n",
        "\n",
        "# Tokenize the dataset sentences and create input tensors\n",
        "inputs = tokenizer(dataset_sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "predictions = torch.argmax(logits, dim=1)\n",
        "\n",
        "accuracy = (predictions == torch.tensor(dataset_labels)).sum().item() / len(dataset_labels)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXVcjXSJ6Zmp"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "hvNvLoeZdRxo",
        "outputId": "5b091f95-f265-4c2e-d7c2-db94db36100f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv02 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-e50fae6b3049>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# Read sentences from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/USER/Desktop/WSD_corpus_context.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/USER/Desktop/WSD_corpus_context.csv'"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "model_path = 'aubmindlab/bert-base-arabertv02'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)  # assuming 2 classes\n",
        "\n",
        "def combine_context_glosses(context, glosses):\n",
        "    # Combine context with each gloss\n",
        "    combined_pairs = []\n",
        "    for gloss in glosses:\n",
        "        # Gloss ID starts with 302 for Ghani and 303 for Modern lexicon\n",
        "        lexicon_source = 'Ghani' if gloss.startswith('302') else 'Modern'\n",
        "\n",
        "        # Split the gloss into concept ID and percentage\n",
        "        concept_id, percentage = gloss.split('@')\n",
        "\n",
        "        # Combine context and gloss as a pair\n",
        "        pair = {\n",
        "            'context': context,\n",
        "            'lexicon_source': lexicon_source,\n",
        "            'concept_id': concept_id,\n",
        "            'percentage': percentage\n",
        "        }\n",
        "        combined_pairs.append(pair)\n",
        "\n",
        "    return combined_pairs\n",
        "\n",
        "def perform_inference(sentence, glosses):\n",
        "    # Combine context and glosses\n",
        "    pairs = combine_context_glosses(sentence, glosses)\n",
        "\n",
        "    # Perform inference for each pair\n",
        "    logits_list = []\n",
        "    for pair in pairs:\n",
        "        # Create input tensors for the pair\n",
        "        inputs = tokenizer(pair['context'], pair['concept_id'], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        # Perform inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        logits_list.append(logits)\n",
        "\n",
        "    return logits_list\n",
        "\n",
        "# Read sentences from CSV\n",
        "sentences = []\n",
        "with open('C:/Users/USER/Desktop/WSD_corpus_context.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        sentences.append(row[0])\n",
        "\n",
        "# Read glosses from CSV\n",
        "glosses = []\n",
        "with open('C:/Users/USER/Desktop/lemmas.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        glosses.append(row[0])\n",
        "\n",
        "# Example usage\n",
        "for sentence in sentences:\n",
        "    logits = perform_inference(sentence, glosses)\n",
        "    # Discuss the ordering of logits or perform further processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "id": "4pW4dPjndpgs",
        "outputId": "fc57b5d3-268f-4463-e3e7-28bcca1f770f"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-fe9b3b07a954>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    return combined_pairs\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def perform_inference(sentence, glosses):\n",
        "    # Combine context and glosses\n",
        "    pairs = combine_context_glosses(sentence, glosses)\n",
        "\n",
        "    # Perform inference for each pair\n",
        "    logits_list = []\n",
        "    for pair in pairs:\n",
        "        # Create input tensors for the pair\n",
        "        inputs = tokenizer(pair['context'], pair['concept_id'], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        # Perform inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        logits_list.append(logits)\n",
        "\n",
        "    return logits_list\n",
        "\n",
        "# Read sentences from CSV\n",
        "sentences = []\n",
        "with open('C:/Users/USER/Desktop/WSD_corpus_context.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        sentences.append(row[0])\n",
        "\n",
        "# Read glosses from CSV\n",
        "glosses = []\n",
        "with open('C:/Users/USER/Desktop/lemmas.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.reader(csvfile)\n",
        "    for row in reader:\n",
        "        glosses.append(row[0])\n",
        "\n",
        "# Example usage\n",
        "for sentence in sentences:\n",
        "    logits = perform_inference(sentence, glosses)\n",
        "    # Discuss the ordering of logits or perform further processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6QR2Ji-UcaS"
      },
      "source": [
        "# Tensor flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMJejXaw92oU",
        "outputId": "d622ddc1-1be3-4807-ccce-e1564cf960d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LTm12eF-5rx",
        "outputId": "8e7a8fb7-434f-431f-9e7e-f1421a5e3cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 17.46 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/aub-mind/arabert.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zUL1_TGAOOv"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# the mock-0.3.1 dir contains testcase.py, testutils.py & mock.py\n",
        "sys.path.append('/content/arabert')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fg_HNV7AXZ2",
        "outputId": "12c773c9-9181-4f10-88b0-216c3082885a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarabic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN6EoRfpAXfJ",
        "outputId": "475758b3-d431-40dc-bbfd-eeca3eccb339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.4)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install farasapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzyI7fXiaMTI",
        "outputId": "c4323d91-71bd-4c1e-ad35-dfb102aea6f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting arabert\n",
            "  Downloading arabert-1.0.1-py3-none-any.whl (179 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyArabic in /usr/local/lib/python3.10/dist-packages (from arabert) (0.6.15)\n",
            "Requirement already satisfied: farasapy in /usr/local/lib/python3.10/dist-packages (from arabert) (0.0.14)\n",
            "Collecting emoji==1.4.2 (from arabert)\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/185.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy->arabert) (4.65.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from PyArabic->arabert) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy->arabert) (3.4)\n",
            "Building wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186458 sha256=37a9d0416270b6e6931299c86e41af94160b4b819fd9b54de91f133f2874268b\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji, arabert\n",
            "Successfully installed arabert-1.0.1 emoji-1.4.2\n"
          ]
        }
      ],
      "source": [
        "pip install arabert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4R-nWoGjm4R",
        "outputId": "973d3d72-bb5b-4ba2-8f32-790167c34ff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.3)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP_jxU3cjKRm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS0SS-Pe95_D",
        "outputId": "ddfeb51c-1f2f-4394-bbe7-6ea80adaf803"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabert were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, BertModel\n",
        "#from arabert.preprocess import never_split_tokens\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "\n",
        "arabert_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabert\",\n",
        "    do_lower_case=False,\n",
        "    do_basic_tokenize=True)\n",
        "arabert_model = AutoModel.from_pretrained(\"aubmindlab/bert-base-arabert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "F_z2xCWo-Dli",
        "outputId": "0a44f64e-e661-45c6-b2c0-19381e5b7a0c"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-863e8928ab68>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0marabert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0marabert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m115\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1615\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'BertModel' object has no attribute 'compile'"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "arabert_model.base_model.compile(optimizer=optimizer, loss=loss)\n",
        "arabert_model.fit( train_data, train_labels, epochs=2, steps_per_epoch=115)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWR5Czw0Ii-I"
      },
      "outputs": [],
      "source": [
        "!pip install tf-models-official"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgX4KLoeIcGF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# import tensorflow_hub as hub\n",
        "# import tensorflow_datasets as tfds\n",
        "# tfds.disable_progress_bar()\n",
        "\n",
        "from official.modeling import tf_utils\n",
        "from official import nlp\n",
        "from official.nlp import bert\n",
        "\n",
        "# Load the required submodules\n",
        "import official.nlp.optimization\n",
        "import official.nlp.bert.bert_models\n",
        "import official.nlp.bert.configs\n",
        "import official.nlp.bert.run_classifier\n",
        "import official.nlp.bert.tokenization\n",
        "import official.nlp.data.classifier_data_lib\n",
        "import official.nlp.modeling.losses\n",
        "import official.nlp.modeling.models\n",
        "import official.nlp.modeling.networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXUZmNhlG8cR"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"/content/drive/MyDrive/masters/new_df_pairs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1UMZ5P-I6kb"
      },
      "outputs": [],
      "source": [
        "new_df_pairs = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GypZTSQuHuOQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkXiwr3iI8KN"
      },
      "outputs": [],
      "source": [
        "gs_folder_bert = '/content/drive/MyDrive/tf_arabert01'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GigJkv7ZJXwx"
      },
      "outputs": [],
      "source": [
        "tokenizer = bert.tokenization.FullTokenizer(\n",
        "    vocab_file=os.path.join(gs_folder_bert, \"vocab.txt\"),\n",
        "     do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up6FNuuA94k4"
      },
      "outputs": [],
      "source": [
        "def encode_sentence(s, tokenizer):\n",
        "   tokens = list(tokenizer.tokenize(s))\n",
        "   tokens.append('[SEP]')\n",
        "   return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer):\n",
        "  num_examples = len(glue_dict[\"sentence1\"])\n",
        "\n",
        "  sentence1 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "      for s in glue_dict[\"sentence1\"]])\n",
        "  sentence2 = tf.ragged.constant([\n",
        "      encode_sentence(s, tokenizer)\n",
        "       for s in glue_dict[\"sentence2\"]])\n",
        "\n",
        "  cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
        "  input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "  input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "  type_cls = tf.zeros_like(cls)\n",
        "  type_s1 = tf.zeros_like(sentence1)\n",
        "  type_s2 = tf.ones_like(sentence2)\n",
        "  input_type_ids = tf.concat(\n",
        "      [type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "  inputs = {\n",
        "      'input_word_ids': input_word_ids.to_tensor(),\n",
        "      'input_mask': input_mask,\n",
        "      'input_type_ids': input_type_ids}\n",
        "\n",
        "  return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRZ8fFd2-Gft"
      },
      "outputs": [],
      "source": [
        "dict_train = {'sentence1':train[\"def\"],\"sentence2\":train[\"ex\"]}\n",
        "train_data_set = bert_encode(dict_train, tokenizer)\n",
        "train_labels = train['label']\n",
        "train_labels = tf.convert_to_tensor(train_labels.tolist())\n",
        "\n",
        "# glue_test = bert_encode(glue['test'], tokenizer)\n",
        "# glue_test_labels  = glue['test']['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIvXXusP_3xi"
      },
      "outputs": [],
      "source": [
        "dict_test = {'sentence1':test[\"def\"],\"sentence2\":test[\"ex\"]}\n",
        "test_data_set = bert_encode(dict_test, tokenizer)\n",
        "test_labels = test['label']\n",
        "test_labels = tf.convert_to_tensor(test_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otHMGsHzLe5N"
      },
      "outputs": [],
      "source": [
        "# def encode_sentence(s):\n",
        "#    tokens = list(tokenizer.tokenize(s))\n",
        "#    tokens.append('[SEP]')\n",
        "#    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# sentence1 = tf.ragged.constant([\n",
        "#     encode_sentence(s) for s in train[\"def\"]])\n",
        "# sentence2 = tf.ragged.constant([\n",
        "#     encode_sentence(s) for s in train[\"ex\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOiCCB3fLgck"
      },
      "outputs": [],
      "source": [
        "# cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*sentence1.shape[0]\n",
        "# input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCVJpcTXLi4l"
      },
      "outputs": [],
      "source": [
        "# input_mask = tf.ones_like(input_word_ids).to_tensor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TklY4SfrLlE2"
      },
      "outputs": [],
      "source": [
        "# type_cls = tf.zeros_like(cls)\n",
        "# type_s1 = tf.zeros_like(sentence1)\n",
        "# type_s2 = tf.ones_like(sentence2)\n",
        "# input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "# # plt.pcolormesh(input_type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abxy8e7gLnnq"
      },
      "outputs": [],
      "source": [
        "import  json\n",
        "bert_config_file = os.path.join(gs_folder_bert, \"config.json\")\n",
        "config_dict = json.loads(tf.io.gfile.GFile(bert_config_file).read())\n",
        "bert_config = bert.configs.BertConfig.from_dict(config_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anMteV2pLp1Y"
      },
      "outputs": [],
      "source": [
        "bert_classifier, bert_encoder = bert.bert_models.classifier_model(\n",
        "    bert_config, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_7WBheVj7k"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l0jzYAA5ioZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj68IldQLsBh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQ6gKnFnLwyi"
      },
      "outputs": [],
      "source": [
        "# train_labels = train['label']\n",
        "# train_labels = tf.convert_to_tensor(train_labels.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYhobuB8L0GE"
      },
      "outputs": [],
      "source": [
        "# train_data= {'input_word_ids':input_word_ids.to_tensor(),\n",
        "#              'input_mask':    input_mask,\"input_type_ids\":  input_type_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMwFTQVtZ41a"
      },
      "outputs": [],
      "source": [
        "# import tensorflow_hub as hub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgA7unvXohjV"
      },
      "outputs": [],
      "source": [
        "!cp  /content/drive/MyDrive/tf_arabert01/arabert01_model.ckpt.index /content/drive/MyDrive/Checkpoints/arabert01_model.ckpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvUciotHexvo"
      },
      "outputs": [],
      "source": [
        "# # tf.compat.v1.train.import_meta_graph('/content/drive/MyDrive/tf_arabert01/arabert01_model.meta')\n",
        "\n",
        "# with tf.compat.v1.Session() as sess:\n",
        "#   new_saver = tf.compat.v1.train.import_meta_graph('/content/drive/MyDrive/tf_arabert01/arabert01_model.meta')\n",
        "#   new_saver.restore(sess, tf.train.latest_checkpoint('/content/drive/MyDrive/tf_arabert01/'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-a3yX3Vle9B"
      },
      "outputs": [],
      "source": [
        "checkpoint =  tf.compat.v1.train.load_checkpoint('/content/drive/MyDrive/tf_arabert01/arabert01_model.ckpt.index')\n",
        "# checkpoint.restore(\"/content/drive/MyDrive/tf_arabert01/arabert01_model.ckpt.index\").expect_partial()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42-KBYrTT-Gg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0D2nexeUKGo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT_XZU2HcPfd"
      },
      "outputs": [],
      "source": [
        "# tf.train.Checkpoint.read(save_path=\"/content/drive/MyDrive/tf_arabert01/\").assert_consumed()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYshallnCWgm"
      },
      "outputs": [],
      "source": [
        "tf.reduce_sum(train_labels).numpy()/len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvVVCMVrL3be"
      },
      "outputs": [],
      "source": [
        "# Set up epochs and steps\n",
        "epochs = 20\n",
        "batch_size = 16\n",
        "eval_batch_size = 15\n",
        "\n",
        "train_data_size = len(train_labels)\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# creates an optimizer with learning rate schedule\n",
        "optimizer = nlp.optimization.create_optimizer(\n",
        "    2e-5*10, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lN9errWwL5Dk"
      },
      "outputs": [],
      "source": [
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "bert_classifier.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=loss,\n",
        "    metrics=metrics)\n",
        "\n",
        "bert_classifier.fit(\n",
        "      train_data_set, train_labels,\n",
        "      validation_data=(test_data_set, test_labels),\n",
        "      batch_size=16,\n",
        "      epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCzSwc5wCaTo"
      },
      "source": [
        "bert_classifier.fit(\n",
        "      train_data_set, train_labels,\n",
        "      validation_data=(test_data_set, test_labels),\n",
        "      batch_size=8,\n",
        "      epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9NP9630Cb7L"
      },
      "outputs": [],
      "source": [
        "# bert_classifier.load_weights('/content/drive/MyDrive/tf_arabert01/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZghMpItSDpAh"
      },
      "outputs": [],
      "source": [
        "# with tf.compat.v1.Session() as sess:\n",
        "#     saver = tf.compat.v1.train.import_meta_graph('/content/drive/MyDrive/tf_arabert01/arabert01_model.meta')\n",
        "#     saver.restore(sess, \"/content/drive/MyDrive/tf_arabert01/arabert01_model.ckpt.index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjVoh4LHL6p8"
      },
      "outputs": [],
      "source": [
        "class BertLayer(tf.compat.v1.layers.Layer):\n",
        "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
        "        self.n_fine_tune_layers = n_fine_tune_layers\n",
        "        self.trainable = True\n",
        "        self.output_size = 768\n",
        "        super(BertLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bert = hub.Module(\n",
        "            bert_path,\n",
        "            trainable=self.trainable,\n",
        "            name=\"{}_module\".format(self.name)\n",
        "        )\n",
        "        trainable_vars = self.bert.variables\n",
        "\n",
        "        # Remove unused layers\n",
        "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
        "\n",
        "        # Select how many layers to fine tune\n",
        "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
        "\n",
        "        # Add to trainable weights\n",
        "        for var in trainable_vars:\n",
        "            self._trainable_weights.append(var)\n",
        "\n",
        "        # Add non-trainable weights\n",
        "        for var in self.bert.variables:\n",
        "            if var not in self._trainable_weights:\n",
        "                self._non_trainable_weights.append(var)\n",
        "\n",
        "        super(BertLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
        "        input_ids, input_mask, segment_ids = inputs\n",
        "        bert_inputs = dict(\n",
        "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
        "        )\n",
        "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
        "            \"pooled_output\"\n",
        "        ]\n",
        "        return result\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], self.output_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxHVzvagYz-c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP1CWx3BWiHe"
      },
      "outputs": [],
      "source": [
        "max_seq_length= 450\n",
        "in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
        "in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
        "in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
        "bert_inputs = [in_id, in_mask, in_segment]\n",
        "\n",
        "# Instantiate the custom Bert Layer defined above\n",
        "bert_output = BertLayer(n_fine_tune_layers=4)(bert_inputs)\n",
        "\n",
        "# Build the rest of the classifier\n",
        "dense = tf.keras.layers.Dense(256, activation='relu')(bert_output)\n",
        "pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=bert_inputs, outputs=pred)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(\n",
        "    [train_input_ids, train_input_masks, train_segment_ids],\n",
        "    train_labels,\n",
        "    validation_data=([test_input_ids, test_input_masks, test_segment_ids], test_labels),\n",
        "    epochs=1,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fxc4d-6_Ywid"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "C2l5__5oWeIN",
        "j6QR2Ji-UcaS"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}